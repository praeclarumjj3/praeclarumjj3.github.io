<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <title>CQFormer</title>
  <link rel="stylesheet" href="./bootstrap.min.css">
  <link rel="stylesheet" href="./temp.css">
  <link rel="stylesheet" href="./CQFormer_style.css">
  <link rel="icon" href="unique.png">
</head>


<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.1043.0" data-gr-ext-installed="">

<div class="container">
  <table border="0" align="center">
    <tbody><tr>
      <td width="1500" height="200" align="center" valign="middle">
      <span class="title"><h1><span style='color:red;'>UNI</span><span style='color:blue;'>QUE</span>: Universal Image Segmentation with Contrastive Query Transformer</h1></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h4>
            <a href="https://praeclarumjj3.github.io/" target="_blank">Jitesh Jain<sup>1,2,3</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
            <a href="https://chrisjuniorli.github.io/" target="_blank">Jiachen Li<sup>1,2*</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
            <a href="https://www.linkedin.com/in/mtchiu/" target="_blank">MangTik Chiu<sup>1*</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
            <a href="https://alihassanijr.com/" target="_blank">Ali Hassani<sup>1,2</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
            <a href="https://www.linkedin.com/in/nukich74/" target="_blank">Nikita Orlov<sup>2</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
            <a href="https://www.humphreyshi.com/home" target="_blank">Humphrey Shi<sup>1,2</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
        </h4></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h6> 
          <sup>1</sup> SHI Lab @ U of Oregon & UIUC &nbsp; &nbsp;   
          <sup>2</sup> Picsart AI Research &nbsp; &nbsp;
          <sup>3</sup> IIT Roorkee &nbsp; &nbsp; 
        </h6> </td>
    </tr>
    <br>
    <tr>
      <td colspan="3" align="center"><h7> <sup>*</sup> Equal Contribution </h7> </td>
  </tr>
    <tr>
      <td colspan="3" align="center"><h5><div class="row justify-content-center" align="center" >
        <a href="https://github.com/SHI-Labs/FcF-Inpainting" target="_blank"> 
          <div class="img-with-text">
            <img src="./cq_paper.png" height="90px" /><br><br>
            <p>ArXiv <br> Paper</p>
          </div>
        </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        
        <a href="https://github.com/SHI-Labs/FcF-Inpainting" target="_blank"> 
          <div class="img-with-text">
            <img src="./github_icon.png" height="90px" /><br><br>
            <p>GitHub <br> Code</p>
          </div>
        </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

        <a href="https://github.com/SHI-Labs/FcF-Inpainting" target="_blank"> 
          <div class="img-with-text">
            <img src="./huggingface_logo.svg" height="90px" /><br><br>
            <p>HuggingFace <br> Space</p>
          </div>
        </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

        <a href="#BibTeX" target="_self"> 
          <div class="img-with-text">
            <img src="./bibtex.png" height="90px" /><br><br>
            <p>Citation <br> BibTeX</p>
          </div>
        </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      
      </div></h5></td>
  </tr>
  </tbody></table>

  

  <br>
  <div class="text" style="text-align: left;">
    <p><img src="./comparison.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:50%" align="center"></p>
      <h2>Abstract</h2>
      <p>image segmentation include scene parsing, panoptic segmentation, and, more recently, universal architectures. However, existing approaches are not truly unified, as they need to train three models individually on the panoptic, instance, and semantic segmentation to achieve the best performance. Ideally, a truly universal framework must only be trained once and achieve SOTA performance on all three image segmentation tasks. To that end, we propose our CQFormer framework, which unifies image segmentation with a multi-task train-once policy, and outperforms existing specialized models on all three tasks. We first propose a task-conditioned joint training strategy that enables training on ground truths of each domain (panoptic, instance, and semantic segmentation) within a single training process. Secondly, we propose using a paired query-text contrastive loss during training to establish better inter-class boundaries and inter-task distinction. Thirdly, we introduce a task-token input to make our model conditioned on the task at hand. Our experiments demonstrate the superior performance of our CQFormer compared to existing individually trained models on three benchmark datasets. Most notably, we achieve significant improvements with Swin-L over Mask2Former on the ADE20K (+0.9% mIoU), Cityscapes (+1.9% AP), and COCO (+0.5% PQ) datasets.</p>
  </div>
</div>

<br>

<div class="container">
  <h2>Method</h2>
  <p><img src="./cqformer.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:80%" class="center"></p>
    <div class="overview">
    <p>
      We propose a truly Universal Task-Conditioned Contrastive Query-based (CQFormer) framework, which when trained
      only once, outperforms the existing state-of-the-art methods on all three image segmentation tasks. 
      We introduce a task input to condition the model on the task in focus, making our architecture task-dynamic. 
      Furthermore, we jointly train our framework by uniformly sampling ground truths from all three image segmentation domains during training.
      Following the recent success of end-to-end query-based transformer frameworks in the computer vision domain, 
      we also formulate our framework as query-based with an additional contrastive loss on the queries during training. 
      Specifically, we initialize our queries as repetitions of the task token and consequently compute a query-text paired
      contrastive loss with the text derived from the corresponding ground-truth label for the sampled task. 
      We hypothesize that a contrastive loss on the queries helps in making the model more task-sensitive.</strong>
    </p>
    </div>
</div>

<!-- <br> -->

<div class="container">
  <h2>Task Conditioned Joint Training</h2>
  <p><img src="./text_gen.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:100%" class="center"></p>
    <div class="pipelines" style="text-align: left;">
      <p>
        We tackle the multi-task train-once challenge for image segmentation using a task-conditioned joint training strategy. 
        Particularly, we first uniformly sample the \(\texttt{task}\) domain \(\in\) {panoptic, semantic, instance} for the GT label. 
        We realize the unification potential of panoptic annotations by deriving the task-specific labels from the panoptic annotations, 
        thus, using only one set of annotations. For e.g., when \(\texttt{task}\) is semantic/instance segmentation, we derive
        the corresponding label from the GT panoptic annotation for the sampled image.
        Next, we extract a set of binary masks for each category present in the image from the task-specific GT label, i.e., 
        semantic task guarantees only one amorphous binary mask for each class present in the image, whereas, instance task 
        signifies non-overlapping binary masks for only thing classes, ignoring the stuff regions. Panoptic task denotes a 
        single amorphous mask for stuff classes and non-overlapping masks for thing classes. Subsequently,
        we iterate over the set of masks to create a list of text entries (\(\mathbf{T}_\text{list}\))
        with a template “a photo with a {\(\texttt{CLS}\)}”, where \(\texttt{CLS}\) is the class name for the corresponding binary mask object. 
        The number of binary masks per sample varies over the dataset. Therefore, we pad(\(\mathbf{T}_\text{list}\)) with “a/an {\(\texttt{task}\)} photo”
        entries to obtain a padded list (\(\mathbf{T}_\text{pad}\)) with text queries of
        constant length. We are motivated to represent the text
        queries as a padded list keeping in mind the meaning of the object queries, which represent the number of objects
        present in an image. We later use (\(\mathbf{T}_\text{pad}\)) to calculate a object text query contrastive loss 
        over the object queries (\(\mathbf{Q}\)) make our model sensitive to the inter-task differences.
      </p>
      <p>
        We condition our architecture on the task using a task input with the template “the task is {\(\texttt{task}\)}”, which
        is tokenized and mapped to task-token (\(\mathbf{Q}_\text{task}\)). We use \(\mathbf{Q}_\text{task}\)
        to initialize and condition the object queries (\(\mathbf{Q}\)) on the \(\texttt{task}\).
      </p>
    </div>
</div>

<div class="container">
  <h2>Results</h2>
  <p><img src="./plots.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:80%" class="center"></p>
    <div class="pipelines" style="text-align: left;">
      <p>
        We experiment on three widely used datasets: COCO, Cityscapes and ADE20K, that support all three: 
        semantic, instance, and panoptic segmentation tasks.
        Our CQFormer, trained only once, outperforms the individually trained Mask2Former (the current SOTA universal architecture).
      </p>
        <details>
          <summary style="font-size:24px;">Task Token</summary>
          <p><img src="./task.png" style="display: block;margin-left: auto;margin-right: auto;max-width:40%" class="center"></p>
          <p>
            We demonstrate that our framework is sensitive to the task token input by setting the value of {\(\texttt{task}\)}
            during inference as panoptic, instance, or semantic. We report results with our Swin-L\(^{\dagger}\) CQFormer trained on ADE20K dataset.
            We observe a significant drop in the PQ and mIoU metrics when \(\texttt{task}\) is instance compared to panoptic. 
            Moreover, the PQ\(^\text{St}\) drops to 0.2%, and there is only a -0.1% drop on PQ\(^\text{Th}\) metric, 
            proving that the network learns to focus majorly on the distinct “thing” instances when the \(\texttt{task}\) is instance. 
            Similarly, there is a sizable drop in the PQ, PQ\(^\textbf{Th}\) and AP metrics for the semantic task with PQ\(^\textbf{St}\) only 
            dropping by -0.2% showing that our framework can segment out amorphous masks for “stuff” regions but does not predict different masks 
            for “thing” objects. Therefore, our model can dynamically learn the task-specific features during training which is critical for a train-once multi-task architecture.
          </p>
        </details>
        <details>
          <summary style="font-size:24px;">Class Consistent Predictions</summary>
          <p><img src="./class_fig.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:100%" class="center"></p>
          <p>Our query-text contrastive
            loss helps us establish an implicit inter-task separation during training and reduces the category inconsistency in the
            final predictions. Mask2Former incorrectly predicts “wall”
            as “fence” in the first row, “vegetation” as “terrain”, and
            “terrain” as “sidewalk”. At the same time, our CQFormer
            produces more accurate predictions in regions (inside <span style='color:blue;'>blue</span>
            boxes) with similar classes.</p>
          </details>
    </div>
</div>

<br>



<!-- </p></div></div><br> -->

<div class="container" id="BibTeX">
  <h2>Citation</h2>
  <p>If you found CQFormer useful in your research, please consider starring ⭐ us on GitHub and citing 📚 us in your research!</p>
  <div style="font-weight:normal; background-color:  #F0F0F0;">
    <tt>
    @article{jain2022unique, <br>
      <div style="margin-left: 5%; font-weight:normal; ">
      title={UNIQUE: Universal Image Segmentation with Contrastive Query Transformer}, <br>
      author={Jitesh Jain and Jiachen Li and MangTik Chiu and Ali Hassani and Nikita Orlov and Humphrey Shi}, <br>
      journal={arXiv}, <br>
      year={2022} <br>
    </div>
    }</tt>
  </div>
</div>

</p></div></div><br>
  

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>