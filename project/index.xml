<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Jitesh Jain</title>
    <link>https://praeclarumjj3.github.io/project/</link>
      <atom:link href="https://praeclarumjj3.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© {2023} Jitesh Jain</copyright><lastBuildDate>Tue, 10 May 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://praeclarumjj3.github.io/media/icon_hud166e63c6d63b20b4040daaa47de5209_74785_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://praeclarumjj3.github.io/project/</link>
    </image>
    
    <item>
      <title>Neural Style Transfer: A Technical Report </title>
      <link>https://praeclarumjj3.github.io/project/nst-tech/</link>
      <pubDate>Tue, 10 May 2022 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/nst-tech/</guid>
      <description>&lt;p&gt;As a part of the &lt;code&gt;CSN-526: Machine Learning&lt;/code&gt; Course Project, we worked on studying the effects of various factors like feature extractors, optimizers, loss functions and initialization techniques on the neural style transfer task.&lt;/p&gt;
&lt;p&gt;We also proposed the use of pseudo ground truths for evaluating the performance under different settings.&lt;/p&gt;
&lt;p&gt;For more info, please see the Code on &lt;code&gt;GitHub&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AOT-GAN Experiments</title>
      <link>https://praeclarumjj3.github.io/project/aot-gan-experiments/</link>
      <pubDate>Wed, 19 May 2021 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/aot-gan-experiments/</guid>
      <description>&lt;p&gt;I conducted various experiments with &lt;strong&gt;AOT-GAN&lt;/strong&gt; proposed in the paper: &lt;a href=&#34;https://arxiv.org/abs/2104.01431&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aggregated Contextual Transformations for High-Resolution Image Inpainting&lt;/a&gt;) on the &lt;a href=&#34;http://places2.csail.mit.edu/download.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Places2&lt;/a&gt; dataset using the &lt;a href=&#34;https://nv-adlr.github.io/publication/partialconv-inpainting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PConv Free Form Masks for Inpainting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In particular, I focused on finding the effectivity of different losses while training the framework. I made several observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;adversarial loss&lt;/strong&gt; doesn&amp;rsquo;t seem to be contributing to the learning of the model as it stays almost the same throughout the training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training for longer than &lt;code&gt;1e4&lt;/code&gt; iterations doesn&amp;rsquo;t add much improvement to the results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training &lt;strong&gt;without style loss&lt;/strong&gt; produces blurry results. Therefore, style loss is an important component for texture related synthesis of images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training &lt;strong&gt;without adversarial loss&lt;/strong&gt; also produces good quality results!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more info, please see the Code on &lt;code&gt;GitHub&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OLIE</title>
      <link>https://praeclarumjj3.github.io/project/olie/</link>
      <pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/olie/</guid>
      <description>&lt;p&gt;OLIE was aimed to reconstruct the original image with the objects removed (editing) by incorporating the mask features as input to the reconstructor.&lt;/p&gt;
&lt;p&gt;After performing many experiments, we scrapped the idea. However, some of the experiments that I performed were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Using a Perturbator Model:&lt;/strong&gt; Taking inspiration from &lt;a href=&#34;https://ganpaint.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GanPaint&lt;/a&gt;, I tried editing images using perturbations in the reconstructor, however it didn&amp;rsquo;t work well.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Using Gated Convolutions:&lt;/strong&gt; Taking inspiration from &lt;a href=&#34;https://paperswithcode.com/paper/free-form-image-inpainting-with-gated&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Free-Form Image Inpainting with Gated Convolution&lt;/a&gt;, I used &lt;code&gt;Gated Modules&lt;/code&gt; instead of standard modules which gave even &lt;strong&gt;poorer results&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Using Two Stage Model:&lt;/strong&gt; Getting hole images from the reconstructor and then passing that into an inpainting model gave somewhat good results as the generating the hole image could be learned accurately to an extent, however, the results were no better than the common inpainting ones.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our aim was to come up with a &lt;strong&gt;Single Stage Model&lt;/strong&gt; for image editing using instance maps.&lt;/p&gt;
&lt;p&gt;For more info, please see the Code on &lt;code&gt;GitHub&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>granim</title>
      <link>https://praeclarumjj3.github.io/project/granim/</link>
      <pubDate>Sat, 03 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/granim/</guid>
      <description>&lt;p&gt;&lt;strong&gt;granim&lt;/strong&gt; makes it easier to plot animated graphs using the Manim-Engine. This project is still under development. Contributors are always welcome!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BackBone Profile</title>
      <link>https://praeclarumjj3.github.io/project/backbone/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/backbone/</guid>
      <description>&lt;p&gt;I performed experiments on various backbone networks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ResNet-18/34/50/101&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MobileNetv2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Xception Net&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the experiments were performed on a single &lt;strong&gt;RTX 2080Ti GPU&lt;/strong&gt;, a single &lt;strong&gt;TitanXP GPU&lt;/strong&gt; and a single &lt;strong&gt;CPU&lt;/strong&gt; &lt;em&gt;indivdually&lt;/em&gt;. For each model architecture, I collected the stats including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Execution Time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory Used&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Number of Parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ArxivApp</title>
      <link>https://praeclarumjj3.github.io/project/arxiv_app/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/arxiv_app/</guid>
      <description>&lt;p&gt;&lt;strong&gt;ArxivApp&lt;/strong&gt; lets you read, bookmark and download papers from arXiv.org. It also has a forum screen that lets you read and create notes/blogs about the different research papers you read!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IITR ChatBot</title>
      <link>https://praeclarumjj3.github.io/project/iitr_bot/</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/iitr_bot/</guid>
      <description>&lt;p&gt;We developed an assistance bot for the &lt;a href=&#34;https://www.iitr.ac.in/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IITR website&lt;/a&gt; using the &lt;a href=&#34;https://rasa.com/docs/rasa/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RASA&lt;/a&gt; framework.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IITR Security App</title>
      <link>https://praeclarumjj3.github.io/project/security_app/</link>
      <pubDate>Sat, 05 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/security_app/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;IITR Security App was developed to eliminate the use of paper by the guards to record the details of the visitors at IIT Roorkee.&lt;/li&gt;
&lt;li&gt;I worked on developing Front-End part of the app using Flutter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Playstore Link&lt;/strong&gt;: &lt;a href=&#34;https://play.google.com/store/apps/details?id=in.ac.iitr.mdg.securityapp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://play.google.com/store/apps/details?id=in.ac.iitr.mdg.securityapp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summaries</title>
      <link>https://praeclarumjj3.github.io/project/summary_of_papers/</link>
      <pubDate>Tue, 07 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/summary_of_papers/</guid>
      <description>&lt;p&gt;Summaries:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/vlgiitr/papers_we_read/blob/master/summaries/You_only_train_once.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You Only Train Once : Loss-conditional training of deep networks, ICLR 2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/vlgiitr/papers_we_read/blob/master/summaries/GrokNet.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GrokNet: Unified Computer Vision Model Trunk and Embeddings For Commerce, KDD 2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Semantically_multi-modal_image_synthesis.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Semantically multi-modal image synthesis, CVPR 2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/vlgiitr/papers_we_read/blob/master/summaries/GameGAN.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Simulate Dynamic Environments with GameGAN, CVPR 2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Adversarial_RL.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adversarial Policies : Attacking deep reinforcement learning, ICLR 2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Unsupervised_learning_for_3D_objects_from_images.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild, CVPR 2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/vlgiitr/papers_we_read/blob/master/summaries/BYOL.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning, CVPR 2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ChatBot with Pytorch</title>
      <link>https://praeclarumjj3.github.io/project/chatbot/</link>
      <pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/chatbot/</guid>
      <description>&lt;p&gt;I implemented a chatbot in Pytorch on the Cornell Movie Dialog Corpus dataset using GRUs and Attention mechanism.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VQ-VAE on MNIST</title>
      <link>https://praeclarumjj3.github.io/project/vq_vae/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/vq_vae/</guid>
      <description>&lt;p&gt;I tried to implement &lt;a href=&#34;https://arxiv.org/abs/1711.00937&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VQ-VAE&lt;/a&gt; in Pytorch on the MNIST dataset.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Time Tracer</title>
      <link>https://praeclarumjj3.github.io/project/time_trace/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/time_trace/</guid>
      <description>&lt;p&gt;A Flutter application developed as a self-learning project to help people keep track of their time expenditure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Societyfy</title>
      <link>https://praeclarumjj3.github.io/project/societyfy/</link>
      <pubDate>Wed, 08 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/societyfy/</guid>
      <description>&lt;p&gt;SOCIETYFY is an app showcase app developed during MDG’s SoC event in December, 2019. Societyfy lets you discover new people based on your interest at any moment of time and make new bonds with people you share an interest with at any point in time through its specific chatrooms for each category.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PlayStore Link&lt;/strong&gt;: &lt;a href=&#34;https://play.google.com/store/apps/details?id=in.ac.mdg.iitr.societyfy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://play.google.com/store/apps/details?id=in.ac.mdg.iitr.societyfy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Medium Blog&lt;/strong&gt;: &lt;a href=&#34;https://medium.com/mobile-development-group/societyfy-an-app-made-for-finding-company-at-anytime-for-anything-842e18151551&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; Societyfy: An app made for finding company at any time for anything&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Geofile</title>
      <link>https://praeclarumjj3.github.io/project/geofile/</link>
      <pubDate>Sun, 05 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/geofile/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;Geofile lets you couple files them with a specific location.&lt;/li&gt;
&lt;li&gt;I worked on developing Front-End part of the app using Flutter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Playstore Early Access&lt;/strong&gt;: You can test the beta version on the Playstore by searching for &lt;strong&gt;geofile&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
