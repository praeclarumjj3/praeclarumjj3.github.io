<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Jitesh Jain</title>
    <link>https://praeclarumjj3.github.io/tag/deep-learning/</link>
      <atom:link href="https://praeclarumjj3.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 19 May 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://praeclarumjj3.github.io/media/icon_hud166e63c6d63b20b4040daaa47de5209_74785_512x512_fill_lanczos_center_3.png</url>
      <title>Deep Learning</title>
      <link>https://praeclarumjj3.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>AOT-GAN Experiments</title>
      <link>https://praeclarumjj3.github.io/project/aot-gan-experiments/</link>
      <pubDate>Wed, 19 May 2021 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/aot-gan-experiments/</guid>
      <description>&lt;p&gt;I conducted various experiments with &lt;strong&gt;AOT-GAN&lt;/strong&gt; proposed in the paper: &lt;a href=&#34;https://arxiv.org/abs/2104.01431&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Aggregated Contextual Transformations for High-Resolution Image Inpainting&lt;/a&gt;) on the &lt;a href=&#34;http://places2.csail.mit.edu/download.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Places2&lt;/a&gt; dataset using the &lt;a href=&#34;https://nv-adlr.github.io/publication/partialconv-inpainting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PConv Free Form Masks for Inpainting&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In particular, I focused on finding the effectivity of different losses while training the framework. I made several observations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;adversarial loss&lt;/strong&gt; doesn&amp;rsquo;t seem to be contributing to the learning of the model as it stays almost the same throughout the training.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training for longer than &lt;code&gt;1e4&lt;/code&gt; iterations doesn&amp;rsquo;t add much improvement to the results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training &lt;strong&gt;without style loss&lt;/strong&gt; produces blurry results. Therefore, style loss is an important component for texture related synthesis of images.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Training &lt;strong&gt;without adversarial loss&lt;/strong&gt; also produces good quality results!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more info, please see the Code on &lt;code&gt;GitHub&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BackBone Profile</title>
      <link>https://praeclarumjj3.github.io/project/backbone/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/backbone/</guid>
      <description>&lt;p&gt;I performed experiments on various backbone networks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ResNet-18/34/50/101&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MobileNetv2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Xception Net&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All the experiments were performed on a single &lt;strong&gt;RTX 2080Ti GPU&lt;/strong&gt;, a single &lt;strong&gt;TitanXP GPU&lt;/strong&gt; and a single &lt;strong&gt;CPU&lt;/strong&gt; &lt;em&gt;indivdually&lt;/em&gt;. For each model architecture, I collected the stats including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Execution Time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory Used&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Number of Parameters&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ChatBot with Pytorch</title>
      <link>https://praeclarumjj3.github.io/project/chatbot/</link>
      <pubDate>Fri, 19 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/chatbot/</guid>
      <description>&lt;p&gt;I implemented a chatbot in Pytorch on the Cornell Movie Dialog Corpus dataset using GRUs and Attention mechanism.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VQ-VAE on MNIST</title>
      <link>https://praeclarumjj3.github.io/project/vq_vae/</link>
      <pubDate>Wed, 20 May 2020 00:00:00 +0000</pubDate>
      <guid>https://praeclarumjj3.github.io/project/vq_vae/</guid>
      <description>&lt;p&gt;I tried to implement &lt;a href=&#34;https://arxiv.org/abs/1711.00937&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VQ-VAE&lt;/a&gt; in Pytorch on the MNIST dataset.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
