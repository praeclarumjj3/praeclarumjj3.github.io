<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>FcF-Inpaining</title>
  <link rel="stylesheet" href="./bootstrap.min.css">
  <link rel="stylesheet" href="./temp.css">
  <link rel="stylesheet" href="./FcF_style.css">
  <link href="icon.png" rel="icon">
</head>


<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.1043.0" data-gr-ext-installed="">

<div class="container">
  <table border="0" align="center">
    <tbody><tr>
      <td width="1500" align="center" valign="middle">
      <span class="title"><h1>Keys to Better Image Inpainting: <br> Structure and Texture Go Hand in Hand</h1></td>
    </tr>
    <tr>
      <td width="1500" align="center" valign="middle">
      <span class="title"><h3>WACV 2023</h3></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h4>
            <a href="https://praeclarumjj3.github.io/" target="_blank">Jitesh Jain<sup>1,2,3*</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
            <a href="https://yzhouas.github.io/" target="_blank">Yuqian Zhou<sup>4*</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
            <a href="https://ningyu1991.github.io/" target="_blank">Ning Yu<sup>5</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
            <a href="https://www.humphreyshi.com/home" target="_blank">Humphrey Shi<sup>1,3</sup></a>  &nbsp;  &nbsp;  &nbsp;  &nbsp;
        </h4></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h6> 
          <sup>1</sup> SHI Lab @ University of Oregon &nbsp; &nbsp;   
          <sup>2</sup> IIT Roorkee &nbsp; &nbsp; 
          <br><br>
          <sup>3</sup> Picsart AI Research &nbsp; &nbsp; 
          <sup>4</sup> Adobe Inc. &nbsp; &nbsp; 
          <sup>5</sup> Salesforce Research &nbsp; &nbsp; 
        </h6> </td>
    </tr>
    <br>
    <tr>
      <td colspan="3" align="center"><h7> <sup>*</sup> Equal Contribution </h7> </td>
  </tr>
    <tr>
      <td colspan="3" align="center"><h5><div class="row justify-content-center" align="center" >
        <a href="https://arxiv.org/abs/2208.03382" target="_blank"> 
          <div class="img-with-text">
            <img src="./fcf_paper.png" height="90px" /><br><br>
            <p>ArXiv <br> Paper</p>
          </div>
        </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        
        <a href="https://github.com/SHI-Labs/FcF-Inpainting" target="_blank"> 
          <div class="img-with-text">
            <img src="./github_icon.png" height="90px" /><br><br>
            <p>GitHub <br> Code</p>
          </div>
        </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

        <a href="https://huggingface.co/spaces/shi-lab/FcF-Inpainting" target="_blank"> 
          <div class="img-with-text">
            <img src="./huggingface_logo.svg" height="90px" /><br><br>
            <p>HuggingFace <br> Space</p>
          </div>
        </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 

        <a href="https://colab.research.google.com/github/SHI-Labs/FcF-Inpainting/blob/main/colab/FcF_Inpainting.ipynb" target="_blank"> 
          <div class="img-with-text">
            <img src="./colab_icon.png" height="90px" /><br><br>
            <p>Colab <br> Demo</p>
          </div>
        </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 

        <a href="#BibTeX" target="_blank"> 
          <div class="img-with-text">
            <img src="./bibtex.png" height="90px" /><br><br>
            <p>Citation <br> BibTeX</p>
          </div>
        </a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      
      </div></h5></td>
  </tr>
  </tbody></table>

  

  <br>
  <div class="text" style="text-align: left;">
    <p><img src="./intro.png" style="display: block;margin-left: auto;margin-right: auto;max-width:90%" align="center"></p>
      <h2>Abstract</h2>
      <p>Deep image inpainting has made impressive progress with recent advances in image generation and processing algorithms. We claim that the performance of inpainting algorithms can be better judged by the generated structures and textures. Structures refer to the generated object boundary or novel geometric structures within the hole, while texture refers to high-frequency details, especially man-made repeating patterns filled inside the structural regions. We believe that better structures are usually obtained from a coarse-to-fine GAN-based generator network while repeating patterns nowadays can be better modeled using state-of-the-art high-frequency fast fourier convolutional layers. In this paper, we propose a novel inpainting network combining the advantages of the two designs. Therefore, our model achieves a remarkable visual quality to match state-of-the-art performance in both structure generation and repeating texture synthesis using a single network. Extensive experiments demonstrate the effectiveness of the method, and our conclusions further highlight the two critical factors of image inpainting quality, structures, and textures, as the future design directions of inpainting networks.</p>
  </div>
</div>

<br>

<div class="container">
  <h2>Method</h2>
  <p><img src="./fcf_gan.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:80%" class="center"></p>
    <div class="overview">
    <p>
      In this paper, we revisited the core design ideas of stateof-the-art deep inpainting networks. 
      We propose an intuitive and effective inpainting architecture that augments the powerful comodulated StyleGAN2 generator with the high receptiveness ability of FFC to achieve equally good performance on both textures and structures.
      Specifically, we generate image structures in a coarse-to-fine StyleGAN-based generation scheme. 
      Meanwhile, we merge between the generated coarse features and the skip features from the encoder and pass them through a Fast Fourier Synthesis (FaF-Syn) module to better generate repeating textures. 
      The convolutional layers inside FaF-Syn are co-modulated using the encoded features and style mapping of the latent noise vector.
      Our idea is simple yet effective, making structures and textures well synthesized within a single network</strong>.
    </p>
    </div>
</div>

<!-- <br> -->

<div class="container">
  <h2>Qualitative Results</h2>
  <p><img src="./places_qual_1.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:100%" class="center"></p>
    <div class="pipelines" style="text-align: left;">
      <p>
        Our model preserves much better repeating textures compared with CoModGAN. CoModGAN does not have any attention-related modules, so
        high-frequency features cannot be effectively reused given
        the limited receptive field. Our model enlarged the receptive field using fast Fourier layers and effectively rendered source textures on newly generated random structures. Meanwhile, ours also outperforms LaMa in generating object boundaries and structures. It is evident that
        LaMa generates fading-out artifacts when the hole reaches
        the image or object boundary. LaMa cannot hallucinate
        good structural information given large holes across longer
        pixel ranges. Ours, however, leverages the advantages of the
        coarse-to-fine generator to synthesize a clear shape boundary of objects in a better manner. In conclusion, our model
        integrates the advantages of two state-of-the-arts and simultaneously generates remarkable structures and textures.
      </p>
      <details>
        <summary style="font-size:24px;">More Scene Completion Results</summary>
        <p><img src="./places_qual_2.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:100%" class="center"></p>
        </details>
        <details>
          <summary style="font-size:24px;">Texture Completion Results</summary>
          <p><img src="./texture_qual.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:100%" class="center"></p>
          </details>
        <details>
          <summary style="font-size:24px;">Face Inpainting Results</summary>
          <p>While testing on face images, especially
            when we covered half of the faces, LaMa generates fadingout hairs on the forehead, and CoModGAN may use others' eyes to complete the images. Though they both obtain
            good numbers in the quantitative results, some drawbacks
            are reflected, making both models not robust enough. Ours
            demonstrates a sound synthesis of hair and forehead shape
            and consistent eye and eyebrow appearance like LaMa. We
            can keep concluding that our proposed model works consistently well on both image structures and consistent textures.</p>
          <p><img src="./face_qual.svg" style="display: block;margin-left: auto;margin-right: auto;max-width:100%" class="center"></p>
          </details>
    </div>
</div>
<br>



<!-- </p></div></div><br> -->

<div class="container" id="BibTeX">
  <h2>Citation</h2>
  <p>If you found the demo useful in your research, please consider starring ‚≠ê us on GitHub and citing üìö us in your research!</p>
  <div style="font-weight:normal; background-color:  #F0F0F0;">
    <tt>
    @inproceedings{jain2022keys, <br>
      <div style="margin-left: 5%; font-weight:normal; ">
      title={Keys to Better Image Inpainting: Structure and Texture Go Hand in Hand}, <br>
      author={Jitesh Jain and Yuqian Zhou and Ning Yu and Humphrey Shi}, <br>
      booktitle={WACV}, <br>
      year={2023} <br>
    </div>
    }</tt>
  </div>
</div>

</p></div></div><br>
  

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>